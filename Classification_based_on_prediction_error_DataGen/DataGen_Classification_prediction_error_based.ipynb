{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sbn\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler,PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from itertools import permutations, combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datafiles to generate data for training of regression and classification model and testing of classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting path for the 'parent folder'\n",
    "path_cwd = os.getcwd()\n",
    "path_parent = os.path.abspath(os.path.join(path_cwd, os.pardir))\n",
    "\n",
    "# Getting path for the data files\n",
    "datafiles_folder_name = 'Data_files'\n",
    "\n",
    "file_train_1 = 'data_base_demand_train_1.csv'\n",
    "file_train_2 = 'data_base_demand_train_2.csv'\n",
    "file_train_3 = 'data_base_demand_train_3.csv'\n",
    "file_test_base_demand = 'data_base_demand_test.csv'\n",
    "\n",
    "\n",
    "file_leak14 = 'data_leak_in_14.csv'\n",
    "file_leak24 = 'data_leak_in_24.csv'\n",
    "file_leak31 = 'data_leak_in_31.csv'\n",
    "\n",
    "path_train1 = os.path.join(path_parent,datafiles_folder_name,file_train_1)\n",
    "path_train2 = os.path.join(path_parent,datafiles_folder_name,file_train_2)\n",
    "path_train3 = os.path.join(path_parent,datafiles_folder_name,file_train_2)\n",
    "path_test_base_demand = os.path.join(path_parent,datafiles_folder_name,file_test_base_demand)\n",
    "\n",
    "path_leak14 = os.path.join(path_parent,datafiles_folder_name,file_leak14)\n",
    "path_leak24 = os.path.join(path_parent,datafiles_folder_name,file_leak24)\n",
    "path_leak31 = os.path.join(path_parent,datafiles_folder_name,file_leak31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proxy for historical observations\n",
    "df_train_reg_model = pd.read_csv(path_train1)\n",
    "df_train_class_model_ref = pd.read_csv(path_train2)\n",
    "df_train_class_model_rec = pd.read_csv(path_train3)\n",
    "\n",
    "# Proxy for recent observations, those need to be tested for leak\n",
    "df_test_class_model = pd.read_csv(path_test_base_demand)\n",
    "leak_file_14 = pd.read_csv(path_leak14)\n",
    "leak_file_24 = pd.read_csv(path_leak24)\n",
    "leak_file_31 = pd.read_csv(path_leak31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the leak datafiles into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leak data to be used for classification model training\n",
    "leaksize_training = [0.0005,0.002,0.003,0.004]\n",
    "leak14_training=leak_file_14.loc[(leak_file_14.leak_area.isin(leaksize_training))]\n",
    "leak24_training=leak_file_24.loc[(leak_file_24.leak_area.isin(leaksize_training))]\n",
    "leak31_training=leak_file_31.loc[(leak_file_31.leak_area.isin(leaksize_training))]\n",
    "\n",
    "# Leak data to be used for classification model testing\n",
    "leaksize_testing = [0.0001,0.001,0.005]\n",
    "leak14_testing=leak_file_14.loc[(leak_file_14.leak_area.isin(leaksize_testing))]\n",
    "leak24_testing=leak_file_24.loc[(leak_file_24.leak_area.isin(leaksize_testing))]\n",
    "leak31_testing=leak_file_31.loc[(leak_file_31.leak_area.isin(leaksize_testing))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to extract X (flow 1, flow 2) and y (deltaH = head 1 - head 2) data for regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(df,link_names,head_names):\n",
    "    \n",
    "    data_flow = np.array(df[link_names])*1000  # convering to litres per sec \n",
    "    data_head = np.array(df[head_names])\n",
    "    \n",
    "\n",
    "    train_out= data_head[:,0] - data_head[:,1] # deltaH\n",
    "    train_in = data_flow                       # flow1, flow2\n",
    "    \n",
    "    return train_in, train_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialising Sensor list and sensor combination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually define a sensor list\n",
    "sensor_list = [[4,4],[9,8],[18,17],[20,20],[26,27],[28,30],[32,33]]\n",
    "\n",
    "# Possible combinations of sensors\n",
    "sen_nums = np.arange(1,len(sensor_list)+1)\n",
    "combs = list(combinations(sen_nums,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "(1, 3)\n",
      "first sensor is  1\n",
      "second sensor is  3\n"
     ]
    }
   ],
   "source": [
    "#chk how combs can help in iterating through all possible combinations of sensors\n",
    "i=1\n",
    "print(len(combs))\n",
    "print(combs[i])\n",
    "print('first sensor is ',combs[i][0])\n",
    "print('second sensor is ',combs[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get X and y data for regression model given sensor pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract input (flow1, flow2) and output (deltaH) data from datasets prepared earlier\n",
    "\n",
    "def data_in_out_noleak(sensor_pair,df):\n",
    "    \n",
    "    h1= sensor_list[sensor_pair[0]-1][0]\n",
    "    h2= sensor_list[sensor_pair[1]-1][0]\n",
    "    f1= sensor_list[sensor_pair[0]-1][1]\n",
    "    f2= sensor_list[sensor_pair[1]-1][1]\n",
    "    \n",
    "    link_name = ['Link_flow'+str(f1),'Link_flow'+str(f2)] \n",
    "    node_name = ['Node_head'+str(h1),'Node_head'+str(h2)]\n",
    "    \n",
    "    data_in, data_out = training_data(df,link_name,node_name)\n",
    "    \n",
    "    return data_in, data_out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract input and output data\n",
    "\n",
    "def data_in_out_withleak(sensor_pair,df):\n",
    "    \n",
    "    h1= sensor_list[sensor_pair[0]-1][0]\n",
    "    h2= sensor_list[sensor_pair[1]-1][0]\n",
    "    f1= sensor_list[sensor_pair[0]-1][1]\n",
    "    f2= sensor_list[sensor_pair[1]-1][1]\n",
    "    \n",
    "    link_name_leak = ['leak_flow_'+str(f1),'leak_flow_'+str(f2)]\n",
    "    node_name_leak = ['leak_head_'+str(h1),'leak_head_'+str(h2)]    \n",
    "    data_in, data_out = training_data(df,link_name_leak,node_name_leak)\n",
    "    \n",
    "    return data_in, data_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression models for each sensor combination being trained and saved as .pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for comb in combs:    \n",
    "    xtrain, ytrain = data_in_out_noleak(comb,df_train_reg_model)    \n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    x_train_poly = poly.fit_transform(xtrain)    \n",
    "    lin_model = LinearRegression()\n",
    "    lin_model.fit(x_train_poly,ytrain)\n",
    "    \n",
    "    pkl_filename = 'linmodel'+str(comb[0])+str(comb[1])+'.pkl'\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(lin_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create column names to be used for the dataframe that will store the results of regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate column names\n",
    "colnames = []\n",
    "for comb in combs:\n",
    "    n1='obs'+str(comb[0])+str(comb[1])\n",
    "    n2='prd'+str(comb[0])+str(comb[1])\n",
    "    n3='stat'+str(comb[0])+str(comb[1])\n",
    "    n4='pval'+str(comb[0])+str(comb[1])\n",
    "    colnames.extend([n1,n2,n3,n4])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The function below creates data to be used for training and testing classification model. It does the following:\n",
    "* For each sensor pair:\n",
    "* Randomly choose a set of observations of specified size from 'historical observations' named df_train_class_model\n",
    "* Extract deltaH from this data which can be referred as deltaH_observed\n",
    "* For the same data, make predictions for deltaH (deltaH_predicted) using the linear regression models stored earlier\n",
    "* Stores the mean of error_historical which is deltaH_observed - deltaH_predicted\n",
    "* Same steps for 'recent observations' from the dataset named df_test_class_model\n",
    "* Error in prediction of recent observation can be referred error_recent\n",
    "* Next the KS test is done on error_historical and error_recent\n",
    "* Stat and pval is calculated and stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def datafortrees(numcases,df_recent,sample_len,casetype):\n",
    "    \n",
    "    df_cases = pd.DataFrame(columns=colnames)\n",
    "    fracsize_recent=sample_len/len(df_recent)\n",
    "    fracsize_reference = sample_len/len(df_train_class_model_ref)\n",
    "    \n",
    "    # first loop to randomly select a sample test set\n",
    "\n",
    "    for i in range(numcases):\n",
    "        df_recent_sample = df_recent.sample(frac=fracsize_recent)\n",
    "        df_reference_sample = df_train_class_model_ref.sample(frac=fracsize_reference)\n",
    "        \n",
    "        # second loop to cover all possible leak combinations\n",
    "\n",
    "        comb_data = []\n",
    "        for comb in combs:\n",
    "            if casetype=='noleak':\n",
    "                xtest_rec, ytest_rec = data_in_out_noleak(comb,df_recent_sample)\n",
    "            else:\n",
    "                xtest_rec, ytest_rec = data_in_out_withleak(comb,df_recent_sample)\n",
    "                \n",
    "            xtest_ref, ytest_ref = data_in_out_noleak(comb,df_reference_sample)\n",
    "            \n",
    "\n",
    "            # load the linear regression model, make predictions and store results\n",
    "            pkl_filename = 'linmodel'+str(comb[0])+str(comb[1])+'.pkl'\n",
    "\n",
    "            with open(pkl_filename, 'rb') as file:\n",
    "                lin_model = pickle.load(file)\n",
    "\n",
    "            xtest_ref_poly = poly.fit_transform(xtest_ref)\n",
    "            xtest_rec_poly = poly.fit_transform(xtest_rec) \n",
    "            pred_ref = lin_model.predict(xtest_ref_poly).reshape(-1)\n",
    "            pred_rec = lin_model.predict(xtest_rec_poly).reshape(-1)\n",
    "            error_ref = (ytest_ref-pred_ref)\n",
    "            error_rec = (ytest_rec-pred_rec)\n",
    "            stat,pval = ks_2samp(error_ref,error_rec)\n",
    "            comb_list = [np.mean(error_ref),np.mean(error_rec),stat,pval]            \n",
    "            comb_data.extend(comb_list)\n",
    "            \n",
    "        comb_series = pd.Series(comb_data,index=df_cases.columns)    \n",
    "        df_cases = df_cases.append(comb_series,ignore_index=True)\n",
    "        \n",
    "    return df_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating dataset for training the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining size of training set. Prefer choosing a multiple of 6 \n",
    "num_samples = 500\n",
    "num_training_sample_total = 3000\n",
    "num_training_sample_leak = int(num_training_sample_total/6)\n",
    "num_training_sample_noleak = num_training_sample_total - 3* (num_training_sample_leak)\n",
    "\n",
    "class_train_noleak = datafortrees(num_training_sample_noleak,df_train_class_model_rec,num_samples,'noleak')\n",
    "class_train_leak14 = datafortrees(num_training_sample_leak,leak14_training,num_samples,'leak')\n",
    "class_train_leak24 = datafortrees(num_training_sample_leak,leak24_training,num_samples,'leak')\n",
    "class_train_leak31 = datafortrees(num_training_sample_leak,leak31_training,num_samples,'leak')\n",
    "\n",
    "class_train_noleak['leak']=0\n",
    "class_train_noleak['leak_num']=0\n",
    "\n",
    "class_train_leak14['leak']=1\n",
    "class_train_leak14['leak_num']=14\n",
    "\n",
    "class_train_leak24['leak']=1\n",
    "class_train_leak24['leak_num']=24\n",
    "\n",
    "class_train_leak31['leak']=1\n",
    "class_train_leak31['leak_num']=31\n",
    "\n",
    "data_train_classification = pd.concat([class_train_noleak,class_train_leak14,\n",
    "                                       class_train_leak24,class_train_leak31],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating dataset for testing the classification model, using the original and different demand distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples=500\n",
    "num_test_sample_total = 1800\n",
    "num_test_sample_leak = int(num_test_sample_total/6)\n",
    "num_test_sample_noleak = num_test_sample_total - 3* (num_test_sample_leak)\n",
    "\n",
    "class_test_noleak = datafortrees(num_test_sample_noleak,df_test_class_model,num_samples,'noleak')\n",
    "\n",
    "class_test_leak14 = datafortrees(num_test_sample_leak,leak14_testing,num_samples,'leak')\n",
    "class_test_leak24 = datafortrees(num_test_sample_leak,leak24_testing,num_samples,'leak')\n",
    "class_test_leak31 = datafortrees(num_test_sample_leak,leak31_testing,num_samples,'leak')\n",
    "\n",
    "class_test_noleak['leak']=0\n",
    "class_test_noleak['leak_num']=0\n",
    "\n",
    "class_test_leak14['leak']=1\n",
    "class_test_leak14['leak_num']=14\n",
    "\n",
    "class_test_leak24['leak']=1\n",
    "class_test_leak24['leak_num']=24\n",
    "\n",
    "class_test_leak31['leak']=1\n",
    "class_test_leak31['leak_num']=31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating one sized leak sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples=500\n",
    "num_test_sample_total = 600\n",
    "num_test_sample_leak = int(num_test_sample_total/6)\n",
    "num_test_sample_noleak = num_test_sample_total - 3* (num_test_sample_leak)\n",
    "\n",
    "class_test_noleak_2 = datafortrees(num_test_sample_noleak,df_test_class_model,num_samples,'noleak')\n",
    "\n",
    "# testing sizes[0.0001,0.001,0.005]\n",
    "\n",
    "leak14_testing_small = leak14_testing.loc[leak14_testing.leak_area==leaksize_testing[0]]\n",
    "leak24_testing_small = leak24_testing.loc[leak24_testing.leak_area==leaksize_testing[0]]\n",
    "leak31_testing_small = leak31_testing.loc[leak31_testing.leak_area==leaksize_testing[0]]\n",
    "\n",
    "leak14_testing_mid = leak14_testing.loc[leak14_testing.leak_area==leaksize_testing[1]]\n",
    "leak24_testing_mid = leak24_testing.loc[leak24_testing.leak_area==leaksize_testing[1]]\n",
    "leak31_testing_mid = leak31_testing.loc[leak31_testing.leak_area==leaksize_testing[1]]\n",
    "\n",
    "leak14_testing_large = leak14_testing.loc[leak14_testing.leak_area==leaksize_testing[2]]\n",
    "leak24_testing_large = leak24_testing.loc[leak24_testing.leak_area==leaksize_testing[2]]\n",
    "leak31_testing_large = leak31_testing.loc[leak31_testing.leak_area==leaksize_testing[2]]\n",
    "\n",
    "class_test_leak14_small = datafortrees(num_test_sample_leak,leak14_testing_small,num_samples,'leak')\n",
    "class_test_leak24_small = datafortrees(num_test_sample_leak,leak24_testing_small,num_samples,'leak')\n",
    "class_test_leak31_small = datafortrees(num_test_sample_leak,leak31_testing_small,num_samples,'leak')\n",
    "\n",
    "class_test_leak14_mid = datafortrees(num_test_sample_leak,leak14_testing_mid,num_samples,'leak')\n",
    "class_test_leak24_mid = datafortrees(num_test_sample_leak,leak24_testing_mid,num_samples,'leak')\n",
    "class_test_leak31_mid = datafortrees(num_test_sample_leak,leak31_testing_mid,num_samples,'leak')\n",
    "\n",
    "class_test_leak14_large = datafortrees(num_test_sample_leak,leak14_testing_large,num_samples,'leak')\n",
    "class_test_leak24_large = datafortrees(num_test_sample_leak,leak24_testing_large,num_samples,'leak')\n",
    "class_test_leak31_large = datafortrees(num_test_sample_leak,leak31_testing_large,num_samples,'leak')\n",
    "\n",
    "class_test_noleak_2['leak']=0\n",
    "class_test_noleak_2['leak_num']=0\n",
    "\n",
    "class_test_leak14_small['leak']=1\n",
    "class_test_leak14_small['leak_num']=14\n",
    "\n",
    "class_test_leak24_small['leak']=1\n",
    "class_test_leak24_small['leak_num']=24\n",
    "\n",
    "class_test_leak31_small['leak']=1\n",
    "class_test_leak31_small['leak_num']=31\n",
    "\n",
    "class_test_leak14_mid['leak']=1\n",
    "class_test_leak14_mid['leak_num']=14\n",
    "\n",
    "class_test_leak24_mid['leak']=1\n",
    "class_test_leak24_mid['leak_num']=24\n",
    "\n",
    "class_test_leak31_mid['leak']=1\n",
    "class_test_leak31_mid['leak_num']=31\n",
    "\n",
    "class_test_leak14_large['leak']=1\n",
    "class_test_leak14_large['leak_num']=14\n",
    "\n",
    "class_test_leak24_large['leak']=1\n",
    "class_test_leak24_large['leak_num']=24\n",
    "\n",
    "class_test_leak31_large['leak']=1\n",
    "class_test_leak31_large['leak_num']=31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating test sets by combining the above created sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test set that includes 'usual demand no leak' and the leak cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_classification_1 = pd.concat([class_test_noleak,class_test_leak14,\n",
    "                                      class_test_leak24,class_test_leak31],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Single leak sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_classification_small_leak = pd.concat([class_test_noleak_2,class_test_leak14_small,\n",
    "                                      class_test_leak24_small,class_test_leak31_small],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_classification_mid_leak = pd.concat([class_test_noleak_2,class_test_leak14_mid,\n",
    "                                      class_test_leak24_mid,class_test_leak31_mid],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_classification_large_leak = pd.concat([class_test_noleak_2,class_test_leak14_large,\n",
    "                                      class_test_leak24_large,class_test_leak31_large],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs12</th>\n",
       "      <th>prd12</th>\n",
       "      <th>stat12</th>\n",
       "      <th>pval12</th>\n",
       "      <th>obs13</th>\n",
       "      <th>prd13</th>\n",
       "      <th>stat13</th>\n",
       "      <th>pval13</th>\n",
       "      <th>obs14</th>\n",
       "      <th>prd14</th>\n",
       "      <th>...</th>\n",
       "      <th>obs57</th>\n",
       "      <th>prd57</th>\n",
       "      <th>stat57</th>\n",
       "      <th>pval57</th>\n",
       "      <th>obs67</th>\n",
       "      <th>prd67</th>\n",
       "      <th>stat67</th>\n",
       "      <th>pval67</th>\n",
       "      <th>leak</th>\n",
       "      <th>leak_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019687</td>\n",
       "      <td>-0.023990</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.010969</td>\n",
       "      <td>0.201138</td>\n",
       "      <td>-0.001874</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.560022</td>\n",
       "      <td>-0.002109</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032839</td>\n",
       "      <td>-0.053999</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.863677</td>\n",
       "      <td>-0.004158</td>\n",
       "      <td>0.018671</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.665922</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012894</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.413486</td>\n",
       "      <td>0.046078</td>\n",
       "      <td>-0.002220</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.902691</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113001</td>\n",
       "      <td>-0.063693</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.034795</td>\n",
       "      <td>0.064291</td>\n",
       "      <td>0.011749</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.291925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004322</td>\n",
       "      <td>0.017788</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.129396</td>\n",
       "      <td>0.135952</td>\n",
       "      <td>-0.115709</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.058689</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030916</td>\n",
       "      <td>-0.051476</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.935112</td>\n",
       "      <td>-0.021604</td>\n",
       "      <td>0.091268</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.095465</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      obs12     prd12  stat12    pval12     obs13     prd13  stat13    pval13  \\\n",
       "0  0.019687 -0.023990   0.102  0.010969  0.201138 -0.001874   0.050  0.560022   \n",
       "1  0.012894  0.009576   0.056  0.413486  0.046078 -0.002220   0.036  0.902691   \n",
       "2  0.004322  0.017788   0.074  0.129396  0.135952 -0.115709   0.084  0.058689   \n",
       "\n",
       "      obs14     prd14  ...     obs57     prd57  stat57    pval57     obs67  \\\n",
       "0 -0.002109  0.000208  ... -0.032839 -0.053999   0.038  0.863677 -0.004158   \n",
       "1 -0.000248  0.001087  ...  0.113001 -0.063693   0.090  0.034795  0.064291   \n",
       "2  0.000148 -0.000720  ... -0.030916 -0.051476   0.034  0.935112 -0.021604   \n",
       "\n",
       "      prd67  stat67    pval67  leak  leak_num  \n",
       "0  0.018671   0.046  0.665922     0         0  \n",
       "1  0.011749   0.062  0.291925     0         0  \n",
       "2  0.091268   0.078  0.095465     0         0  \n",
       "\n",
       "[3 rows x 86 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "data_test_classification_1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the datasets created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "data_train_classification = data_train_classification.sample(frac=1)\n",
    "# Output folder name defined\n",
    "datafiles_folder_name = 'Data_files'\n",
    "\n",
    "# Output file names defined\n",
    "datafile_training = 'data_training_classification_pred_error_based.csv'\n",
    "\n",
    "# Creating file paths. Note that 'path_parent' has been defined earlier\n",
    "path_training = os.path.join(path_parent,datafiles_folder_name,datafile_training)\n",
    "\n",
    "# Creating the 'Data_files' folder if it doesn't exist\n",
    "os.makedirs(os.path.dirname(path_training), exist_ok=True)\n",
    "\n",
    "# Saving the output datasets as csv files whose paths have been defined above\n",
    "data_train_classification.to_csv(path_training, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "data_test_classification_1 = data_test_classification_1.sample(frac=1)\n",
    "datafile_test1 = 'data_testing_classification_pred_error_based_BaseDemand.csv'\n",
    "# Creating file paths. Note that 'path_parent' has been defined earlier\n",
    "path_test1 = os.path.join(path_parent,datafiles_folder_name,datafile_test1)\n",
    "# Saving the output datasets as csv files whose paths have been defined above\n",
    "data_test_classification_1.to_csv(path_test1, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Leak Test set\n",
    "\n",
    "# Output folder name defined\n",
    "datafiles_folder_name = 'Data_files'\n",
    "\n",
    "data_test_classification_small_leak = data_test_classification_small_leak.sample(frac=1)\n",
    "data_test_classification_mid_leak = data_test_classification_mid_leak.sample(frac=1)\n",
    "data_test_classification_large_leak = data_test_classification_large_leak.sample(frac=1)\n",
    "\n",
    "datafile_small = 'data_testing_classification_small_leak.csv'\n",
    "datafile_mid = 'data_testing_classification_mid_leak.csv'\n",
    "datafile_large = 'data_testing_classification_large_leak.csv'\n",
    "\n",
    "# Creating file paths. Note that 'path_parent' has been defined earlier\n",
    "\n",
    "path_test1 = os.path.join(path_parent,datafiles_folder_name,datafile_small)\n",
    "path_test2 = os.path.join(path_parent,datafiles_folder_name,datafile_mid)\n",
    "path_test3 = os.path.join(path_parent,datafiles_folder_name,datafile_large)\n",
    "\n",
    "# Saving the output datasets as csv files whose paths have been defined above\n",
    "\n",
    "data_test_classification_small_leak.to_csv(path_test1, index=None)\n",
    "data_test_classification_mid_leak.to_csv(path_test2, index=None)\n",
    "data_test_classification_large_leak.to_csv(path_test3, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plots showing the prediction error profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n",
      "first sensor is  4\n",
      "second sensor is  5\n"
     ]
    }
   ],
   "source": [
    "comb_num = 15 # an integer between 1 and 21\n",
    "\n",
    "print(combs[comb_num])\n",
    "print('first sensor is ',combs[comb_num][0])\n",
    "print('second sensor is ',combs[comb_num][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
