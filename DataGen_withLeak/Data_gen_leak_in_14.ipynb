{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import combinations, combinations_with_replacement, product\n",
    "import wntr\n",
    "import random\n",
    "import networkx as nx\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting path for the 'parent folder'\n",
    "path_cwd = os.getcwd()\n",
    "path_parent = os.path.abspath(os.path.join(path_cwd, os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting path for the input file\n",
    "inputfiles_folder_name = 'Input_files_EPANET'\n",
    "filename = 'Hanoi_leak_14.inp'\n",
    "path_file = os.path.join(path_parent,inputfiles_folder_name,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the input file into EPANET\n",
    "inp_file = path_file\n",
    "wn = wntr.network.WaterNetworkModel(inp_file)\n",
    "wn1 = wntr.network.WaterNetworkModel(inp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting path for the 'No leak datafile' that contains expected states of given nodal demands and resulting flows and pressures\n",
    "# across the WDN\n",
    "data_folder_name = 'Data_files'\n",
    "data_filename = 'data_base_demand_leakgen.csv'\n",
    "path_data_file = os.path.join(path_parent,data_folder_name,data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating empty dataframes with suitably named columns, to be used to store the outputs with leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we run an EPANET simulation only to create sample output from a simulation\n",
    "# and store the column headers of those results. These headers will be used in a file to store results from the \n",
    "# \"leak experiment\" \n",
    "\n",
    "sim_leak = wntr.sim.EpanetSimulator(wn1)\n",
    "results = sim_leak.run_sim()\n",
    "df_demand = results.node['demand']\n",
    "df_head = results.node['head']\n",
    "df_flow = results.link['flowrate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the column names from the results in cell above and create empty dataframes to store results of \n",
    "# leak experiment \n",
    "demand_data = pd.DataFrame(columns = df_demand.columns)\n",
    "leak_demand_data = pd.DataFrame(columns = df_demand.columns)\n",
    "head_data = pd.DataFrame(columns = df_demand.columns)\n",
    "flow_data = pd.DataFrame(columns = df_flow.columns)\n",
    "\n",
    "# 'Expected states' stand for 'No Leak' demand cases that are being used for leak simulation.\n",
    "# The underlying demand, resulting flow and pressure without any leak are stored along with the \n",
    "# resulting flow and pressure with leak.\n",
    "\n",
    "expected_states = pd.read_csv(path_data_file)\n",
    "expected_states = expected_states.sample(frac=0.25)\n",
    "expected_states = expected_states.reset_index(drop=True)\n",
    "expected_state_data = pd.DataFrame(columns=expected_states.columns)\n",
    "expected_states_np = np.array(expected_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing some key values\n",
    "num_nodes = 32\n",
    "num_links = 34\n",
    "train_samples = expected_states_np.shape[0] # No of demand scenarios to be fed to EPANET with leak in place\n",
    "num_total_leaks = 1 # For current set of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of names of all nodes and links\n",
    "link_name = wn1.link_name_list\n",
    "node_name = wn1.node_name_list\n",
    "leak_node_name = node_name[num_nodes-1:-1] # The leak node is assigned number one greater than the last normal node\n",
    "leaking_node = pd.DataFrame(columns = leak_node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating more empty dataframes to store results: one for storing the demand scenario being fed into EPANET during\n",
    "# the experiment, another to store the area of the leak(s) that are active during the experiment\n",
    "demand_data_in = pd.DataFrame(columns = df_demand.columns[:-1])\n",
    "area_in = pd.DataFrame(columns=leak_node_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next two functions are used to for creating leak combinations in case of multiple leaks and for assigning 'area' to the leaks, which is assumed to be a circular orifice. Note that multiple leaks at a time along with multiple possible leak sizes would lead to a large number of possible combinations. In the current study, we have assumed only one leak at a time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below code creates a list of nCp combinations for 'n' links with leak nodes and 'p' total leaks in the system. \n",
    "# Here we have assumed that EACH PIPE CAN HAVE ONLY ONE LEAK AT A TIME. Since we have only one leak, \n",
    "# its a single element list.\n",
    "def leak_combs(num_tot_leaks):\n",
    "         \n",
    "    combs = list(combinations(leak_node_name,num_tot_leaks))\n",
    "    \n",
    "    return combs                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given combination of leak nodes, say 2 leak nodes, each node may have a range of leak areas. The code\n",
    "# below first creates a list of areas and then creates nPc combinations where 'n' is the no of areas and 'c' \n",
    "# is no of leaks\n",
    "def leak_area_combs(area_list):    \n",
    "    combs = list(combinations_with_replacement(area_list,num_total_leaks))    \n",
    "    return combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A hole of radius 2 cm would have an area of around 1e-3 m2 and hole with radius 12 cm would have \n",
    "# around 50e-3 m2. We are assuming a leak in this area range. Default emmitter coefficient is 0.75 \n",
    "\n",
    "area_list = leak_area_combs([0.0001, 0.0005, 0.001 , 0.002 , 0.003 , 0.004 , 0.005 ])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0001,), (0.0005,), (0.001,), (0.002,), (0.003,), (0.004,), (0.005,)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loop below takes in the demand values from each of the 'No Leak datafile' ('expected states in the code below'). These demand values are then fed into the EPANET along with a leak. However, for a given demand scenario, multiple leak sizes are used. The resulting flow and head values are stored along with the leak size and leak demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "for i in range(train_samples):\n",
    "    wn = wntr.network.WaterNetworkModel(inp_file)\n",
    "    # set up the reservoir head as per 'No leak datafile'\n",
    "    wn.get_node(1).head_timeseries.base_value = expected_states_np[i,63]\n",
    "    \n",
    "    # set up demands across junction nodes as per 'No leak datafile'\n",
    "    \n",
    "    for n in range(2,num_nodes+1):            \n",
    "        wn.get_node(n).demand_timeseries_list[0].base_value = expected_states_np[i,n-2]       \n",
    "  \n",
    "        \n",
    "    # Loop 1: over each of the leakage node combinations in the network\n",
    "    for leak_nodes in leak_combs(num_total_leaks):           \n",
    "\n",
    "        # Loop 3: over each of the Emitter coefficient value combination (its a fixed list given no of leaks)\n",
    "        for a in range(len(area_list)):\n",
    "                        \n",
    "            wn_sim = copy.deepcopy(wn)\n",
    "            leaking_node_data = [] # to store leak node names and corresponding ECs\n",
    "\n",
    "            # Loop 4: over each of the leakage node for a leakage node combination and EC combination\n",
    "            for b in range(num_total_leaks):\n",
    "                wn_sim.get_node(leak_nodes[b]).add_leak(wn_sim,area = area_list[a][b], start_time=0)                \n",
    "                leaking_node_data.append(leak_nodes[b])\n",
    "                leaking_node_data.append(area_list[a][b])                     \n",
    "\n",
    "            l_leak = []\n",
    "            for leaknode in leak_node_name:\n",
    "                l_leak.append(wn_sim.get_node(leaknode).leak_area)\n",
    "            area_in.loc[k] = l_leak                \n",
    "\n",
    "            sim = wntr.sim.WNTRSimulator(wn_sim)\n",
    "            results = sim.run_sim()\n",
    "            demand_data.loc[k] = results.node['demand'].loc[0]\n",
    "            leak_demand_data.loc[k] = results.node['leak_demand'].loc[0]\n",
    "            head_data.loc[k] = results.node['head'].loc[0]\n",
    "            flow_data.loc[k] = results.link['flowrate'].loc[0]\n",
    "            expected_state_data.loc[k] = expected_states.loc[i]                     \n",
    "\n",
    "            k=k+1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below stores the data generated by simulator in a single dataframe. Naming of the columns is important and need to be taken care of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_demand = demand_data.copy()\n",
    "leak_demand_leak = leak_demand_data.copy()\n",
    "leak_flow = flow_data.copy()\n",
    "leak_head = head_data.copy()\n",
    "leak_area = area_in.copy()\n",
    "leak_expected = expected_state_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "leakheadnames = []\n",
    "for x in list(leak_head):\n",
    "    y = 'leak_head_'+x\n",
    "    leakheadnames.append(y)\n",
    "    \n",
    "leak_head.columns = leakheadnames\n",
    "\n",
    "leakflownames = []\n",
    "for x in list(leak_flow.columns):\n",
    "    y = 'leak_flow_'+x\n",
    "    leakflownames.append(y)\n",
    "    \n",
    "leak_flow.columns=leakflownames\n",
    "\n",
    "leak_area.columns=['leak_area']\n",
    "\n",
    "leakage_demand = pd.DataFrame(columns=['leakage_demand'])\n",
    "leakage_demand['leakage_demand'] = leak_demand_leak['33']\n",
    "\n",
    "leak_combined = pd.concat((leak_area,leakage_demand,leak_head,leak_flow,leak_expected),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the dataframe as csv in the Data_files folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output folder name defined\n",
    "datafiles_folder_name = 'Data_files'\n",
    "\n",
    "# Output file names defined\n",
    "leakfile = 'data_leak_in_14.csv'\n",
    "\n",
    "# Creating file paths. Note that 'path_parent' has been defined earlier\n",
    "path_leak_file = os.path.join(path_parent,datafiles_folder_name,leakfile)\n",
    "\n",
    "# Creating the 'Data_files' folder, if it doesn't already exists\n",
    "os.makedirs(os.path.dirname(path_leak_file), exist_ok=True)\n",
    "\n",
    "# Saving the output datasets as csv files whose paths have been defined above\n",
    "leak_combined.to_csv(path_leak_file,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17500, 168)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check shape\n",
    "leak_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
