{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import wntr\n",
    "import networkx as nx\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating file path names and reading the EPANET input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting path for the 'parent folder'\n",
    "path_cwd = os.getcwd()\n",
    "path_parent = os.path.abspath(os.path.join(path_cwd, os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting path for the input file\n",
    "inputfiles_folder_name = 'Input_files_EPANET'\n",
    "filename = 'Hanoi_base_demand.inp'\n",
    "path_file = os.path.join(path_parent,inputfiles_folder_name,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the input file into EPANET\n",
    "inp_file = path_file\n",
    "wn = wntr.network.WaterNetworkModel(inp_file)\n",
    "wn1 = wntr.network.WaterNetworkModel(inp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting base demands and reservoir head from the inp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store no of nodes and links\n",
    "num_nodes = len(wn1.node_name_list)\n",
    "num_links = len(wn1.link_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array that contains the base reservoir head and demand at nodes\n",
    "base_demands = np.zeros((num_nodes))\n",
    "base_demands[0]= wn1.get_node(1).head_timeseries.base_value\n",
    "for i in range(1,num_nodes):\n",
    "    base_demands[i] = wn1.get_node(i+1).demand_timeseries_list[0].base_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.000e+02 2.472e-01 2.361e-01 3.611e-02 2.014e-01 2.792e-01 3.750e-01 1.528e-01 1.458e-01 1.458e-01 1.389e-01 1.556e-01 2.611e-01 1.708e-01 7.778e-02 8.611e-02 2.403e-01 3.736e-01 1.667e-02 3.542e-01 2.583e-01 1.347e-01 2.903e-01 2.278e-01 4.722e-02 2.500e-01 1.028e-01 8.056e-02 1.000e-01\n",
      " 1.000e-01 2.917e-02 2.236e-01]\n"
     ]
    }
   ],
   "source": [
    "#check\n",
    "print(base_demands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating co variance matrices\n",
    "* Function to create a cov mat given the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define standard deviation matrix\n",
    "std_dev = base_demands*0.2\n",
    "std_dev[0] = base_demands[0]*0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.000e+01, 4.944e-02, 4.722e-02, 7.222e-03, 4.028e-02, 5.583e-02, 7.500e-02, 3.056e-02, 2.917e-02, 2.917e-02, 2.778e-02, 3.111e-02, 5.222e-02, 3.417e-02, 1.556e-02, 1.722e-02, 4.806e-02, 7.472e-02, 3.334e-03, 7.083e-02, 5.167e-02, 2.694e-02, 5.806e-02, 4.556e-02, 9.444e-03, 5.000e-02,\n",
       "       2.056e-02, 1.611e-02, 2.000e-02, 2.000e-02, 5.834e-03, 4.472e-02])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a covariance matrix \n",
    "def cov_mat_fixed(corr_demands,corr_reservoir_nodes):\n",
    "    \n",
    "    N = num_nodes\n",
    "    \n",
    "    mat_corr = np.zeros((N,N)) # initializing matrix\n",
    "\n",
    "    mat = np.full((N-1,N-1),corr_demands) # step 1 for a symmetric matrix of n-1 by n-1\n",
    "    mat_symm = (mat + mat.T)/2           # step 2\n",
    "\n",
    "    diag = np.ones(N)   # setting up the diagonal matrix, variance of nodal demands\n",
    "    np.fill_diagonal(mat_symm,diag)    \n",
    "    mat_corr[1:,1:] = mat_symm\n",
    "\n",
    "    mat_corr[0,0] = 1          # element (0,0) which is variance of resevoir head\n",
    "\n",
    "    top = np.full((N-1),corr_reservoir_nodes) # covariance between reservoir head and nodal demands\n",
    "    mat_corr[0,1:]=top\n",
    "    mat_corr[1:,0]=top\n",
    "    \n",
    "    Diag = np.diag(std_dev)\n",
    "    cov_mat = Diag * mat_corr * Diag\n",
    "    \n",
    "    return cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covmat for all experiments\n",
    "covmat_base = cov_mat_fixed(0.6,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for the dataframe that will store results. Note that the Reservoir Demand and Head comes after all\n",
    "# other nodes. Naming is done accordingly.\n",
    "\n",
    "col_names=['Demand'+str(i) for i in range(2,33)]+\\\n",
    "          ['Demand_Reservoir']+\\\n",
    "          ['Node_head'+str(i) for i in range(2,33)]+\\\n",
    "          ['Res_Head']+\\\n",
    "          ['Link_flow'+str(i) for i in range(1,35)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "# total columns = 32 demands + 32 heads + 34 flows\n",
    "len(col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating set of base demands and reservoir head that are higher than the 'expected' or 'usual' base case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 1\n",
    "demand1_1 = base_demands*(1+.01)\n",
    "demand1_2 = base_demands*(1+.02)\n",
    "demand1_3 = base_demands*(1+.03)\n",
    "demand1_4 = base_demands*(1+.04)\n",
    "demand1_5 = base_demands*(1+.05)\n",
    "\n",
    "# Set 2\n",
    "demand2_1 = base_demands*(1+.06)\n",
    "demand2_2 = base_demands*(1+.07)\n",
    "demand2_3 = base_demands*(1+.08)\n",
    "demand2_4 = base_demands*(1+.09)\n",
    "demand2_5 = base_demands*(1+.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation(train_samples,cov_mat,nodes_data):\n",
    "      \n",
    "    #getting samples \n",
    "    train_data_raw = np.random.multivariate_normal(nodes_data,cov_mat,train_samples)   \n",
    "    \n",
    "    #removing samples with negative values    \n",
    "    train_data_raw_positive = train_data_raw[train_data_raw.min(axis=1)>=0,:]   \n",
    "    \n",
    "    #creating numpy arrays to store EPANET simulation output  \n",
    "    train_samples_positive = train_data_raw_positive.shape[0]  \n",
    "    data_out_demand = np.zeros((train_samples_positive,num_nodes))\n",
    "    data_out_head = np.zeros((train_samples_positive,num_nodes))\n",
    "    data_out_flow = np.zeros((train_samples_positive,num_links))   \n",
    "    \n",
    "    # loop to simulation to generate heads and flowrates\n",
    "    for i in range(train_samples_positive):\n",
    "        \n",
    "        # updating reservoir head in the epanet input \n",
    "        wn.get_node(1).head_timeseries.base_value = train_data_raw_positive[i,0]\n",
    "        \n",
    "        # updating nodal demand for all nodes in the epanet input \n",
    "        j=1\n",
    "        for n in wn.nodes.junction_names:            \n",
    "            wn.get_node(n).demand_timeseries_list[0].base_value = train_data_raw_positive[i,j]            \n",
    "            j=j+1\n",
    "  \n",
    "        # running epanet simulator\n",
    "        sim = wntr.sim.EpanetSimulator(wn)\n",
    "        \n",
    "        # storing simulation results in 3 matrices\n",
    "        results = sim.run_sim()\n",
    "        data_out_demand[i] = np.array(results.node['demand'])\n",
    "        data_out_head[i] = np.array(results.node['head'])\n",
    "        data_out_flow[i] = np.array(results.link['flowrate'])\n",
    "    \n",
    "    # joining the above 3 matrices\n",
    "    data_out_all = np.hstack((data_out_demand,data_out_head,data_out_flow))\n",
    "    \n",
    "    # dropping the results where any of the nodal pressure head value is negative\n",
    "    data_out_all_positive = data_out_all[data_out_all[:,num_nodes:2*num_nodes].min(axis=1)>=0,:]\n",
    "    \n",
    "    return pd.DataFrame(columns=col_names,data=data_out_all_positive, index= None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating output sets for testing classification model for case of 'different underlying demand scenario'. Datasets are created in small sizes and clubbed to make 2 sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_1 = data_generation(2000,covmat_base,demand1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_2 = data_generation(2000,covmat_base,demand1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_3 = data_generation(2000,covmat_base,demand1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_4 = data_generation(2000,covmat_base,demand1_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_5 = data_generation(2000,covmat_base,demand1_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 98) (2000, 98) (2000, 98) (2000, 98) (2000, 98)\n"
     ]
    }
   ],
   "source": [
    "# check size\n",
    "print(data1_1.shape,data1_2.shape,data1_3.shape,data1_4.shape,data1_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 98)\n"
     ]
    }
   ],
   "source": [
    "data1 = pd.concat([data1_1,data1_2,data1_3,data1_4,data1_5],axis=0)\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_1 = data_generation(2000,covmat_base,demand2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_2 = data_generation(2000,covmat_base,demand2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_3 = data_generation(2000,covmat_base,demand2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_4 = data_generation(2000,covmat_base,demand2_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_5 = data_generation(2000,covmat_base,demand2_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 98) (2000, 98) (2000, 98) (2000, 98) (2000, 98)\n"
     ]
    }
   ],
   "source": [
    "# check size\n",
    "print(data2_1.shape,data2_2.shape,data2_3.shape,data2_4.shape,data2_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 98)\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.concat([data2_1,data2_2,data2_3,data2_4,data2_5],axis=0)\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the output sets as csv datafiles to a folder named 'Data_files'. The datafiles are named while saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output folder name defined\n",
    "datafiles_folder_name = 'Data_files'\n",
    "\n",
    "# Output file names defined\n",
    "datafile_1 = 'data_diff_demand_low.csv'\n",
    "datafile_2 = 'data_diff_demand_high.csv'\n",
    "\n",
    "# Creating file paths. Note that 'path_parent' has been defined earlier\n",
    "path_data_file_1 = os.path.join(path_parent,datafiles_folder_name,datafile_1)\n",
    "path_data_file_2 = os.path.join(path_parent,datafiles_folder_name,datafile_2)\n",
    "\n",
    "\n",
    "# Creating the 'Data_files' folder, if it doesn't already exists\n",
    "os.makedirs(os.path.dirname(path_data_file_1), exist_ok=True)\n",
    "\n",
    "# Saving the output datasets as csv files whose paths have been defined above\n",
    "data1.to_csv(path_data_file_1,index=None)\n",
    "data2.to_csv(path_data_file_2,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
